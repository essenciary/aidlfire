# AIDL Project Guide - Fire Detection from Satellite Imagery

A comprehensive guide to the CEMS Wildfire Dataset and how to use it for deep learning fire segmentation.

---

## Table of Contents

- [The Big Picture](#the-big-picture)
- [What's in the Data?](#whats-in-the-data)
  - [Satellite Images (Multiple Channels)](#satellite-images-multiple-channels)
  - [Fire Masks (Labels)](#fire-masks-labels)
- [Why Do We Cut Images into Patches?](#why-do-we-cut-images-into-patches)
- [Classification vs Segmentation](#classification-vs-segmentation)
- [The Training Process](#the-training-process)
- [What About Inference?](#what-about-inference)
- [Where Are the Labels?](#where-are-the-labels)
- [Training Data vs Real-Time Inference](#training-data-vs-real-time-inference)
- [Understanding the GRA (Severity) Mask](#understanding-the-gra-severity-mask)
- [Why Cloud Masks Matter](#why-cloud-masks-matter)
- [Images with Zero Burned Pixels](#images-with-zero-burned-pixels)
- [The Complete Pipeline](#the-complete-pipeline)
- [Summary](#summary)

---

## The Big Picture

Imagine you have satellite photos of forests that caught fire. Your goal is to teach a computer to look at a satellite photo and say "fire happened here, here, and here" - like coloring in the burned areas.

```
SATELLITE PHOTO              YOUR MODEL'S JOB
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸŒ²ğŸŒ²ğŸ”¥ğŸ”¥ğŸŒ²ğŸŒ²   â”‚         â”‚  â¬œâ¬œğŸŸ¥ğŸŸ¥â¬œâ¬œ   â”‚
â”‚  ğŸŒ²ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸŒ²   â”‚   â”€â”€â–º   â”‚  â¬œğŸŸ¥ğŸŸ¥ğŸŸ¥ğŸŸ¥â¬œ   â”‚
â”‚  ğŸŒ²ğŸŒ²ğŸ”¥ğŸ”¥ğŸŒ²ğŸŒ²   â”‚         â”‚  â¬œâ¬œğŸŸ¥ğŸŸ¥â¬œâ¬œ   â”‚
â”‚  ğŸŒ²ğŸŒ²ğŸŒ²ğŸŒ²ğŸŒ²ğŸŒ²   â”‚         â”‚  â¬œâ¬œâ¬œâ¬œâ¬œâ¬œ   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Input Image               Output Mask
                            (red = burned)
```

---

## What's in the Data?

### Satellite Images (Multiple Channels)

Regular photos have 3 channels: Red, Green, Blue (RGB). But satellite images are special - they capture light our eyes can't see:

```
Normal Photo: 3 channels (RGB)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ Red â”‚Greenâ”‚Blue â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

Satellite Image: 12 channels!
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚Blue â”‚Greenâ”‚ Red â”‚ ... â”‚ NIR â”‚ ... â”‚SWIR1â”‚SWIR2â”‚ ... â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
  â–²                        â–²           â–²     â–²
  â”‚                        â”‚           â””â”€â”€â”€â”€â”€â”´â”€â”€ These "see" burned areas!
  â”‚                        â”‚                     (infrared light)
  â””â”€â”€ Visible light        â””â”€â”€ Near-infrared
      (what we see)            (plants reflect this)
```

**Why so many channels?** Different wavelengths reveal different things:
- **SWIR (Short-Wave Infrared)**: Burned areas show up really clearly here - like a superpower for detecting fire damage
- **NIR (Near-Infrared)**: Healthy plants reflect this strongly, dead/burned plants don't

### Fire Masks (Labels)

For every satellite image, experts have drawn where the fire burned. This is your "answer key":

**DEL (Delineation)** - Binary: burned or not burned
```
0 = not burned (background)
1 = burned (fire area)
```

**GRA (Grading)** - Severity levels
```
0 = no damage
1 = minimal damage
2 = moderate damage
3 = high damage
4 = destroyed
```

---

## Why Do We Cut Images into Patches?

### Problem: Images Are HUGE

A single satellite image might be 1500 Ã— 1500 pixels or bigger. Neural networks:
1. Need consistent input sizes (like 256 Ã— 256)
2. Would run out of memory with huge images
3. Learn better from many small examples than few big ones

### Solution: Slice It Up!

```
Original Image (1500 Ã— 1500)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”       â”‚
â”‚    â”‚  1  â”‚  2  â”‚  3  â”‚  4  â”‚       â”‚
â”‚    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤       â”‚
â”‚    â”‚  5  â”‚  6  â”‚  7  â”‚  8  â”‚  ...  â”‚
â”‚    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤       â”‚
â”‚    â”‚  9  â”‚ 10  â”‚ 11  â”‚ 12  â”‚       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Each patch = 256 Ã— 256 pixels
```

### Why Overlap During Training?

We use 50% overlap (stride of 128 pixels) during training:

```
Without overlap:        With 50% overlap:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1  â”‚  2  â”‚          â”‚  1  â”¼â”€â”€2    â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤          â”‚  â”¼â”€â”€â”¼â”€â”€â”¼    â”‚
â”‚  3  â”‚  4  â”‚          â”‚  3  â”¼â”€â”€4    â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
4 patches              More patches, edges
                       seen multiple times
```

**Why overlap?**
- A fire at the edge of patch 1 appears in the middle of patch 2
- The model sees each area from different "contexts"
- It's like data augmentation - more training examples!

---

## Classification vs Segmentation

### Binary Classification (Simpler)

**Question**: "Does this patch contain ANY fire?"
**Answer**: Yes (1) or No (0)

```
Input: 256Ã—256Ã—7 image
Output: Single number (0 or 1)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸŒ²ğŸ”¥ğŸŒ²ğŸŒ²ğŸŒ²  â”‚
â”‚ ğŸŒ²ğŸ”¥ğŸ”¥ğŸŒ²ğŸŒ²  â”‚  â”€â”€â–º Model â”€â”€â–º "Yes, there's fire" (1)
â”‚ ğŸŒ²ğŸŒ²ğŸŒ²ğŸŒ²ğŸŒ²  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

You could do this by checking: "if any pixel in mask > 0, label = 1"

### Segmentation (What We Actually Want)

**Question**: "Which PIXELS are burned?"
**Answer**: A mask the same size as the input

```
Input: 256Ã—256Ã—7 image
Output: 256Ã—256 mask (one prediction per pixel!)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸŒ²ğŸ”¥ğŸŒ²ğŸŒ²ğŸŒ²  â”‚         â”‚ 0 1 0 0 0   â”‚
â”‚ ğŸŒ²ğŸ”¥ğŸ”¥ğŸŒ²ğŸŒ²  â”‚  â”€â”€â–º    â”‚ 0 1 1 0 0   â”‚
â”‚ ğŸŒ²ğŸŒ²ğŸŒ²ğŸŒ²ğŸŒ²  â”‚         â”‚ 0 0 0 0 0   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Input Image           Output Mask
```

**Segmentation is harder** because you're making thousands of predictions (one per pixel) instead of just one.

---

## The Training Process

### What the Neural Network Sees

```
Input Tensor Shape: (batch_size, 7, 256, 256)
                         â”‚    â”‚    â”‚
                         â”‚    â”‚    â””â”€â”€ Width & Height
                         â”‚    â””â”€â”€ 7 spectral bands (we pick the useful ones)
                         â””â”€â”€ Multiple images at once (batch)

Target Mask Shape: (batch_size, 256, 256)
                         â”‚       â”‚
                         â”‚       â””â”€â”€ Same size as input
                         â””â”€â”€ One label per pixel (0, 1, 2, 3, or 4)
```

### Training Loop (Simplified)

```python
for each batch of patches:

    # 1. Forward pass: model makes predictions
    predictions = model(images)      # Shape: (batch, num_classes, 256, 256)

    # 2. Compare to ground truth
    loss = compare(predictions, masks)  # How wrong are we?

    # 3. Backpropagation: figure out how to improve
    loss.backward()

    # 4. Update model weights
    optimizer.step()
```

### What the Model Learns

The model learns patterns like:
- "When SWIR bands show high values + NIR shows low values â†’ probably burned"
- "This texture pattern usually means fire damage"
- "Edges between these colors often indicate burn boundaries"

---

## What About Inference?

**Inference** = using your trained model on new images

### Does the Input Look the Same?

**YES!** The input format is identical:
- Same 7 bands (or 12 if you use all)
- Same 256Ã—256 patch size
- Same value range (0-1)

```
Training:                          Inference:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Image Patch  â”‚                  â”‚ NEW Image    â”‚
â”‚ (7, 256,256) â”‚                  â”‚ (7, 256,256) â”‚
â”‚      +       â”‚                  â”‚              â”‚
â”‚ Ground Truth â”‚    â”€â”€â”€â”€â”€â”€â–º       â”‚ No labels!   â”‚
â”‚ Mask         â”‚    (trained      â”‚ Model        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     model)       â”‚ predicts     â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Inference on a Full Image

Since real images are big, you:
1. Cut the new image into patches (no overlap needed, or small overlap)
2. Run each patch through the model
3. Stitch the predictions back together

```
New Satellite Image          Run Model on Each          Stitch Together
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1  â”‚  2  â”‚  3  â”‚         â”‚pred1â”‚ â”‚pred2â”‚  ...    â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤   â”€â”€â–º   â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜   â”€â”€â–º   â”‚  Full Predicted â”‚
â”‚  4  â”‚  5  â”‚  6  â”‚         â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”         â”‚      Mask       â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜         â”‚pred4â”‚ â”‚pred5â”‚         â”‚                 â”‚
                            â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Where Are the Labels?

The **labels are the mask files** (DEL and GRA). For every satellite image, there's a corresponding mask that shows where fire burned:

```
For each satellite image:
â”œâ”€â”€ EMSR230_AOI01_01_S2L2A.tif    â† INPUT (what satellite sees)
â”œâ”€â”€ EMSR230_AOI01_01_DEL.tif      â† LABEL (binary: fire/no fire)
â”œâ”€â”€ EMSR230_AOI01_01_GRA.tif      â† LABEL (severity: 0-4)
â””â”€â”€ EMSR230_AOI01_01_CM.tif       â† Cloud mask (helper data)
```

After patching:
```
â”œâ”€â”€ patch_r0_c0_image.npy         â† INPUT  (256Ã—256Ã—7 channels)
â””â”€â”€ patch_r0_c0_mask.npy          â† LABEL  (256Ã—256 with 0/1 or 0-4)
```

### How Was the Data Labeled?

**NOT by an algorithm** â€” by human experts at Copernicus Emergency Management Service (CEMS):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LABELING PROCESS (done by CEMS analysts)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. Fire event reported (e.g., Portugal 2017)                   â”‚
â”‚                         â†“                                       â”‚
â”‚  2. Analysts get PRE-fire satellite image                       â”‚
â”‚                         â†“                                       â”‚
â”‚  3. Analysts get POST-fire satellite image                      â”‚
â”‚                         â†“                                       â”‚
â”‚  4. Analysts MANUALLY draw polygons around burned areas         â”‚
â”‚     (comparing pre vs post, using spectral signatures)          â”‚
â”‚                         â†“                                       â”‚
â”‚  5. Polygons converted to raster masks (DEL, GRA)               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This is why the labels are high quality â€” they're **human-verified ground truth**, not automated predictions.

### What the Model Learns

The model learns the **relationship between spectral patterns and fire presence**:

```
TRAINING (what you have):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Satellite Image â”‚     â”‚   Ground Truth   â”‚
â”‚  (7 channels)    â”‚ â”€â”€â–º â”‚   Mask (DEL)     â”‚
â”‚                  â”‚     â”‚   0 = no fire    â”‚
â”‚  INPUT           â”‚     â”‚   1 = fire       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“                        â†“
    Model learns:  "when pixels look like THIS â†’ label is THAT"


INFERENCE (real-time):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Satellite Image â”‚     â”‚        ?         â”‚
â”‚  (7 channels)    â”‚ â”€â”€â–º â”‚   YOUR MODEL     â”‚
â”‚                  â”‚     â”‚   PREDICTS THIS  â”‚
â”‚  INPUT           â”‚     â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Training Data vs Real-Time Inference

### Key Differences

| Aspect | Training Data | Real-Time Inference |
|--------|---------------|---------------------|
| **Satellite image** | âœ… Yes | âœ… Yes |
| **Mask (labels)** | âœ… Yes (DEL/GRA) | âŒ **NO** â€” this is what you predict! |
| **Cloud mask** | âœ… Provided | âš ï¸ Need to generate or estimate |
| **When acquired** | Post-fire (burned area visible) | During or right after fire |
| **Quality** | Curated, cloud-free selected | Whatever is available |
| **Time context** | Historical events | Live/current events |

### What Real-Time Data is Missing

When you use your trained model on new satellite images:

1. **No mask** â€” that's your model's job to predict!
2. **No curated cloud mask** â€” you may need to:
   - Use Sentinel-2's built-in cloud detection
   - Train a separate cloud detection model
   - Use external cloud masking services
3. **No guaranteed quality** â€” training images were selected for good visibility; real-time may have:
   - Partial cloud cover
   - Smoke from active fires
   - Haze or atmospheric interference
4. **No geographic metadata** â€” you'll need to track coordinates yourself

### The Inference Pipeline You'll Build

```
Real-time Sentinel-2 image
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Download from ESA   â”‚  (Copernicus Open Access Hub)
â”‚    Sentinel-2 API      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Select same 7 bands â”‚  indices (1,2,3,7,8,10,11)
â”‚    Normalize to 0-1    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Generate cloud mask â”‚  (optional but recommended)
â”‚    or estimate clouds  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Cut into 256Ã—256    â”‚  stride=256 (no overlap needed)
â”‚    patches             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Run YOUR MODEL      â”‚  â†’ predicted mask per patch
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Stitch patches back â”‚  â†’ full image prediction
â”‚    into full image     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Fire/burn map!
```

### Key Takeaway

Your trained model learns patterns like "low NIR + high SWIR = burned area" from labeled training data. At inference time, it applies these learned patterns to new, unlabeled images to predict where fires/burns are located â€” **no labels needed**.

---

## Understanding the GRA (Severity) Mask

### Why Does the TIF Look Black?

The TIF file contains integer values **0, 1, 2, 3, 4**. When your image viewer opens it:

```
TIF values:     0    1    2    3    4
                â”‚    â”‚    â”‚    â”‚    â”‚
                â–¼    â–¼    â–¼    â–¼    â–¼
Image viewer    â–     â–     â–     â–     â–      (all look black!)
expects 0-255
```

Since 0-4 are all near zero on a 0-255 scale, they appear black. The **data is correct**, it's just not visible without scaling.

### PNG vs TIF

The PNG is just a **visualization** with a colormap applied:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Value â”‚ Meaning     â”‚ PNG Color       â”‚ RGB              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   0   â”‚ No damage   â”‚ â¬› Black        â”‚ (0, 0, 0)        â”‚
â”‚   1   â”‚ Negligible  â”‚ ğŸŸ© Light Green  â”‚ (181, 254, 142)  â”‚
â”‚   2   â”‚ Moderate    â”‚ ğŸŸ¨ Yellow       â”‚ (254, 217, 142)  â”‚
â”‚   3   â”‚ High        â”‚ ğŸŸ§ Orange       â”‚ (254, 153, 41)   â”‚
â”‚   4   â”‚ Destroyed   â”‚ ğŸŸ¥ Dark Red     â”‚ (204, 76, 2)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**For training, use the TIF** (with values 0-4). The PNG is just for humans to look at.

### Not All Images Have GRA

Some fires only have DEL (binary burned/not burned), while some have both DEL and GRA (severity levels). This depends on what CEMS analysts provided. The `satelliteData.csv` has a `GRA` column (0 or 1) indicating availability.

### Visualizing the TIF Properly

```python
import rasterio
import matplotlib.pyplot as plt

with rasterio.open("path/to/GRA.tif") as src:
    gra = src.read(1)

# Now you can see it!
plt.imshow(gra, cmap='YlOrRd', vmin=0, vmax=4)
plt.colorbar(label='Severity (0-4)')
plt.show()
```

---

## Why Cloud Masks Matter

### The Problem

When the satellite takes a photo, it captures whatever is there - including clouds:

```
What satellite sees:          What's actually on the ground:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸŒ²ğŸŒ²ğŸŒ²â˜ï¸â˜ï¸â˜ï¸ğŸŒ²ğŸŒ²â”‚       â”‚ ğŸŒ²ğŸŒ²ğŸŒ²ğŸ”¥ğŸ”¥ğŸ”¥ğŸŒ²ğŸŒ²â”‚
â”‚ ğŸŒ²ğŸŒ²â˜ï¸â˜ï¸â˜ï¸â˜ï¸ğŸŒ²ğŸŒ²â”‚       â”‚ ğŸŒ²ğŸŒ²ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸŒ²ğŸŒ²â”‚
â”‚ ğŸŒ²ğŸŒ²ğŸŒ²ğŸŒ²ğŸ”¥ğŸ”¥ğŸŒ²ğŸŒ²â”‚       â”‚ ğŸŒ²ğŸŒ²ğŸŒ²ğŸŒ²ğŸ”¥ğŸ”¥ğŸŒ²ğŸŒ²â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     S2L2A image               Reality (hidden by cloud)

The satellite image has cloud pixels where fire actually exists!
```

**The cloud pixels in S2L2A contain cloud reflectance, NOT ground information.** That data is useless for fire detection.

### How Cloud Mask Helps

```
S2L2A (satellite image)       CM (cloud mask)           What to trust
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸŒ²ğŸŒ²ğŸŒ²â˜ï¸â˜ï¸â˜ï¸ğŸŒ²ğŸŒ²â”‚      â”‚ 0 0 0 1 1 1 0 0 â”‚   â”‚ âœ“ âœ“ âœ“ âœ— âœ— âœ— âœ“ âœ“ â”‚
â”‚ ğŸŒ²ğŸŒ²â˜ï¸â˜ï¸â˜ï¸â˜ï¸ğŸŒ²ğŸŒ²â”‚  +   â”‚ 0 0 1 1 1 1 0 0 â”‚ = â”‚ âœ“ âœ“ âœ— âœ— âœ— âœ— âœ“ âœ“ â”‚
â”‚ ğŸŒ²ğŸŒ²ğŸŒ²ğŸŒ²ğŸ”¥ğŸ”¥ğŸŒ²ğŸŒ²â”‚      â”‚ 0 0 0 0 0 0 0 0 â”‚   â”‚ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              0 = clear, 1 = cloud      âœ— = ignore these pixels
```

### Think of it Like This

Imagine you're grading a student's answers, but someone spilled coffee on part of the paper:

- **S2L2A** = the paper (some parts have coffee stains)
- **Cloud mask** = shows you WHERE the coffee stains are
- **DEL/GRA** = the answer key

You wouldn't grade the coffee-stained parts - you'd skip them. Same with clouds!

### In Practice

**During training:**
- Skip patches with too many clouds (>50% cloudy = unreliable)
- Or mask out cloudy pixels from the loss calculation

**During inference:**
- Flag predictions in cloudy areas as "uncertain"
- Or don't make predictions for those pixels

```python
# Example: mask out cloudy pixels during training
cloud_mask = load("CM.tif")  # 0=clear, 1+=cloud
is_clear = (cloud_mask == 0)

# Only compute loss on clear pixels
loss = criterion(prediction[is_clear], target[is_clear])
```

---

## Images with Zero Burned Pixels

### Why Do Some Images Have No Fire?

About 4% of images have `pixelBurned = 0`. When a large fire is mapped, CEMS divides it into **tiles** (like a grid). Some tiles at the edges might not contain any actual fire:

```
Fire Event EMSR207 - Area of Interest 01
Divided into 9 tiles:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  tile_01â”‚  tile_02â”‚  tile_03â”‚
â”‚  ğŸ”¥ğŸ”¥   â”‚  ğŸ”¥ğŸ”¥ğŸ”¥ â”‚         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  tile_04â”‚  tile_05â”‚  tile_06â”‚
â”‚  ğŸ”¥ğŸ”¥ğŸ”¥ â”‚  ğŸ”¥ğŸ”¥   â”‚         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  tile_07â”‚  tile_08â”‚  tile_09â”‚   â—„â”€â”€ tile_07 has 0 burned pixels!
â”‚         â”‚  ğŸ”¥     â”‚         â”‚       (it's in the dataset but no fire)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Should You Keep Them?

| Approach | Pros | Cons |
|----------|------|------|
| Keep all | Model learns "no fire" cases | Class imbalance if too many |
| Remove all | Only train on fire images | Model might predict fire everywhere |
| Keep some | Balanced training | Need to decide how many |

**Recommendation:** For segmentation, having some negative patches (no fire) is actually useful so the model learns to predict "0" when there's no fire.

---

## The Complete Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DATA PREPARATION                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Raw Satellite Images        Cut into Patches         Ready for PyTorch â”‚
â”‚  (huge GeoTIFFs)            (256Ã—256 chunks)          (numpy arrays)    â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”          image.npy          â”‚
â”‚  â”‚             â”‚            â”‚     â”‚ â”‚     â”‚          mask.npy           â”‚
â”‚  â”‚  1500Ã—1500  â”‚    â”€â”€â–º     â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤    â”€â”€â–º    metadata.csv        â”‚
â”‚  â”‚  12 bands   â”‚            â”‚     â”‚ â”‚     â”‚                             â”‚
â”‚  â”‚             â”‚            â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              TRAINING                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Load Patches          Feed to Model           Update Weights           â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ image   â”‚          â”‚             â”‚         â”‚ Loss: 0.5   â”‚          â”‚
â”‚  â”‚ (7,256, â”‚   â”€â”€â–º    â”‚   U-Net     â”‚   â”€â”€â–º   â”‚ Loss: 0.3   â”‚          â”‚
â”‚  â”‚  256)   â”‚          â”‚   or other  â”‚         â”‚ Loss: 0.1   â”‚          â”‚
â”‚  â”‚         â”‚          â”‚   segmenter â”‚         â”‚     â†“       â”‚          â”‚
â”‚  â”‚ mask    â”‚          â”‚             â”‚         â”‚ Model gets  â”‚          â”‚
â”‚  â”‚(256,256)â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ better!     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                             INFERENCE                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  New Image             Trained Model            Prediction              â”‚
â”‚  (never seen!)         (frozen weights)         (fire map!)             â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ new     â”‚          â”‚             â”‚         â”‚ â¬œâ¬œğŸŸ¥ğŸŸ¥â¬œ   â”‚          â”‚
â”‚  â”‚ image   â”‚   â”€â”€â–º    â”‚   U-Net     â”‚   â”€â”€â–º   â”‚ â¬œğŸŸ¥ğŸŸ¥ğŸŸ¥â¬œ   â”‚          â”‚
â”‚  â”‚ (same   â”‚          â”‚  (trained)  â”‚         â”‚ â¬œâ¬œğŸŸ¥â¬œâ¬œ   â”‚          â”‚
â”‚  â”‚ format) â”‚          â”‚             â”‚         â”‚             â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                         â”‚
â”‚  NO MASK NEEDED!                               "Fire is HERE"           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary

| Concept | What It Means |
|---------|---------------|
| **Satellite bands** | Different "colors" including invisible light - SWIR sees fire damage |
| **Patching** | Cutting big images into small pieces the neural network can handle |
| **Overlap** | Patches share edges during training = more data, better learning |
| **DEL mask** | Binary answer: burned (1) or not (0) |
| **GRA mask** | Severity: how badly burned (0-4) |
| **Cloud mask** | Shows which pixels are blocked by clouds (unreliable data) |
| **Classification** | "Is there fire in this patch?" â†’ Yes/No |
| **Segmentation** | "Where exactly is the fire?" â†’ pixel-by-pixel map |
| **Inference** | Using trained model on new images (same format, no labels needed) |

---

## Key Files Reference

```
Each image directory contains:
â”œâ”€â”€ *_S2L2A.tif      â—„â”€â”€ INPUT: Satellite image (12 bands)
â”œâ”€â”€ *_DEL.tif        â—„â”€â”€ TARGET: Binary fire mask (0 or 1)
â”œâ”€â”€ *_GRA.tif        â—„â”€â”€ TARGET: Severity mask (0-4) [not always present]
â”œâ”€â”€ *_CM.tif         â—„â”€â”€ FILTER: Cloud mask (0=clear, 1+=cloud)
â”œâ”€â”€ *_ESA_LC.tif     â—„â”€â”€ EXTRA: Land cover type
â””â”€â”€ *.png            â—„â”€â”€ Visualizations (for humans, not training)
```

**The magic**: Once trained, your model can look at a satellite image it's never seen and draw the fire boundaries automatically!
