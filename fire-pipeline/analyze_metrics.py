#!/usr/bin/env python3
"""
Analyze training metrics for fire detection / segmentation.

Reads metrics.csv generated by MetricLogger and produces
publication-ready plots. Supports comparing multiple models.
"""

from pathlib import Path
import argparse
from typing import Dict, List

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np


def plot_metric(
    df: pd.DataFrame,
    metric: str,
    title: str,
    ylabel: str,
    output_dir: Path,
):
    """Generic metric vs epoch plot."""
    plt.figure(figsize=(7, 4))
    plt.plot(df["epoch"], df[metric], marker="o")
    plt.xlabel("Epoch")
    plt.ylabel(ylabel)
    plt.title(title)
    plt.grid(True)
    plt.tight_layout()

    output_path = output_dir / f"{metric}.png"
    plt.savefig(output_path, dpi=150)
    plt.close()

    print(f"Saved: {output_path}")


def plot_metric_comparison(
    models_data: Dict[str, pd.DataFrame],
    metric: str,
    title: str,
    ylabel: str,
    output_dir: Path,
    split: str = "val",
):
    """Plot same metric across multiple models for comparison."""
    plt.figure(figsize=(10, 6))
    
    for model_name, df in models_data.items():
        if metric not in df.columns:
            print(f"  Warning: Metric '{metric}' not found in {model_name}")
            continue
        
        plt.plot(df["epoch"], df[metric], marker="o", label=model_name, linewidth=2)
    
    plt.xlabel("Epoch")
    plt.ylabel(ylabel)
    plt.title(f"{title} ({split.upper()})")
    plt.legend(loc="best")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    output_path = output_dir / f"{metric}_comparison.png"
    plt.savefig(output_path, dpi=150)
    plt.close()

    print(f"Saved: {output_path}")


def plot_multiple_metrics_comparison(
    models_data: Dict[str, pd.DataFrame],
    metrics: List[str],
    titles: List[str],
    ylabels: List[str],
    output_dir: Path,
    split: str = "val",
):
    """Plot multiple metrics in a grid for comparison."""
    n_metrics = len(metrics)
    n_cols = 2
    n_rows = (n_metrics + 1) // 2
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 5 * n_rows))
    axes = axes.flatten()
    
    for idx, (metric, title, ylabel) in enumerate(zip(metrics, titles, ylabels)):
        ax = axes[idx]
        
        for model_name, df in models_data.items():
            if metric not in df.columns:
                continue
            
            ax.plot(df["epoch"], df[metric], marker="o", label=model_name, linewidth=2)
        
        ax.set_xlabel("Epoch")
        ax.set_ylabel(ylabel)
        ax.set_title(f"{title} ({split.upper()})")
        ax.legend(loc="best")
        ax.grid(True, alpha=0.3)
    
    # Hide unused subplots
    for idx in range(n_metrics, len(axes)):
        axes[idx].set_visible(False)
    
    fig.tight_layout()
    output_path = output_dir / f"metrics_comparison_grid.png"
    plt.savefig(output_path, dpi=150)
    plt.close()

    print(f"Saved: {output_path}")


def print_comparison_summary(
    models_data: Dict[str, pd.DataFrame],
    metrics: List[str],
):
    """Print summary statistics for each model."""
    print("\n" + "=" * 80)
    print("MODEL COMPARISON SUMMARY")
    print("=" * 80)
    
    for model_name, df in models_data.items():
        print(f"\n{model_name}:")
        print("-" * 40)
        
        for metric in metrics:
            if metric in df.columns:
                max_val = df[metric].max()
                max_epoch = df.loc[df[metric].idxmax(), "epoch"]
                print(f"  {metric:20s}: {max_val:.4f} (epoch {int(max_epoch)})")


def main():
    parser = argparse.ArgumentParser(
        description="Analyze fire detection training metrics with multi-model comparison"
    )
    parser.add_argument(
        "--metrics",
        type=Path,
        action="append",
        dest="metrics_files",
        help="Path to metrics.csv (can specify multiple with --metrics file1.csv --metrics file2.csv)",
    )
    parser.add_argument(
        "--model-names",
        type=str,
        action="append",
        dest="model_names",
        help="Model name for legend (use in order with --metrics)",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("output/plots"),
        help="Directory to save plots",
    )
    parser.add_argument(
        "--split",
        type=str,
        default="val",
        choices=["train", "val"],
        help="Which split to analyze",
    )
    parser.add_argument(
        "--compare",
        action="store_true",
        help="Enable multi-model comparison plots (requires multiple --metrics)",
    )
    parser.add_argument(
        "--single",
        action="store_true",
        help="Generate single-model plots (use with single --metrics file)",
    )

    args = parser.parse_args()

    args.output_dir.mkdir(parents=True, exist_ok=True)

    # Handle case: single metrics file (backwards compatible)
    if args.metrics_files is None or len(args.metrics_files) == 0:
        # Default single file mode
        args.metrics_files = [Path("output/metrics.csv")]
        args.model_names = None
        args.single = True

    # Load metrics
    models_data = {}
    
    for idx, metrics_file in enumerate(args.metrics_files):
        if not metrics_file.exists():
            print(f"Warning: {metrics_file} not found, skipping")
            continue
        
        df = pd.read_csv(metrics_file)
        
        if args.split not in df["split"].unique():
            raise ValueError(f"Split '{args.split}' not found in {metrics_file}")
        
        df = df[df["split"] == args.split].sort_values("epoch")
        
        # Determine model name
        if args.model_names and idx < len(args.model_names):
            model_name = args.model_names[idx]
        else:
            # Use parent directory as model name
            model_name = metrics_file.parent.name
        
        models_data[model_name] = df
        print(f"Loaded {len(df)} epochs for '{model_name}' from {metrics_file}")

    if len(models_data) == 0:
        print("ERROR: No metrics loaded")
        return

    # Single model mode: generate individual plots
    if args.single or len(models_data) == 1:
        print("\n" + "=" * 60)
        print("SINGLE MODEL ANALYSIS")
        print("=" * 60 + "\n")
        
        df = list(models_data.values())[0]
        
        # -------------------------
        # SEGMENTATION METRICS
        # -------------------------
        if "fire_iou" in df:
            plot_metric(
                df,
                metric="fire_iou",
                title="Fire IoU (Validation)",
                ylabel="IoU",
                output_dir=args.output_dir,
            )

        if "fire_dice" in df:
            plot_metric(
                df,
                metric="fire_dice",
                title="Fire Dice (Validation)",
                ylabel="Dice",
                output_dir=args.output_dir,
            )

        if "mean_iou" in df:
            plot_metric(
                df,
                metric="mean_iou",
                title="Mean IoU (Validation)",
                ylabel="IoU",
                output_dir=args.output_dir,
            )

        # -------------------------
        # DETECTION METRICS
        # -------------------------
        if "detection_f1" in df:
            plot_metric(
                df,
                metric="detection_f1",
                title="Fire Detection F1 (Validation)",
                ylabel="F1 Score",
                output_dir=args.output_dir,
            )

        if "detection_recall" in df:
            plot_metric(
                df,
                metric="detection_recall",
                title="Fire Detection Recall (Validation)",
                ylabel="Recall",
                output_dir=args.output_dir,
            )

        if "detection_precision" in df:
            plot_metric(
                df,
                metric="detection_precision",
                title="Fire Detection Precision (Validation)",
                ylabel="Precision",
                output_dir=args.output_dir,
            )

        # -------------------------
        # LOSS
        # -------------------------
        if "loss" in df:
            plot_metric(
                df,
                metric="loss",
                title="Loss (Validation)",
                ylabel="Loss",
                output_dir=args.output_dir,
            )

        print("\nSingle model analysis complete.")

    # Multi-model comparison mode
    if args.compare or len(models_data) > 1:
        if len(models_data) < 2:
            print("\nSkipping comparison plots (need at least 2 models)")
        else:
            print("\n" + "=" * 60)
            print("MULTI-MODEL COMPARISON")
            print("=" * 60 + "\n")

            # -------------------------
            # SEGMENTATION METRICS COMPARISON
            # -------------------------
            if all("fire_iou" in df.columns for df in models_data.values()):
                plot_metric_comparison(
                    models_data,
                    metric="fire_iou",
                    title="Fire IoU",
                    ylabel="IoU",
                    output_dir=args.output_dir,
                    split=args.split,
                )

            if all("fire_dice" in df.columns for df in models_data.values()):
                plot_metric_comparison(
                    models_data,
                    metric="fire_dice",
                    title="Fire Dice",
                    ylabel="Dice",
                    output_dir=args.output_dir,
                    split=args.split,
                )

            # -------------------------
            # DETECTION METRICS COMPARISON
            # -------------------------
            if all("detection_f1" in df.columns for df in models_data.values()):
                plot_metric_comparison(
                    models_data,
                    metric="detection_f1",
                    title="Fire Detection F1",
                    ylabel="F1 Score",
                    output_dir=args.output_dir,
                    split=args.split,
                )

            if all("detection_recall" in df.columns for df in models_data.values()):
                plot_metric_comparison(
                    models_data,
                    metric="detection_recall",
                    title="Fire Detection Recall",
                    ylabel="Recall",
                    output_dir=args.output_dir,
                    split=args.split,
                )

            # -------------------------
            # LOSS COMPARISON
            # -------------------------
            if all("loss" in df.columns for df in models_data.values()):
                plot_metric_comparison(
                    models_data,
                    metric="loss",
                    title="Training Loss",
                    ylabel="Loss",
                    output_dir=args.output_dir,
                    split=args.split,
                )

            # -------------------------
            # GRID COMPARISON (all metrics at once)
            # -------------------------
            metrics_to_plot = ["fire_iou", "detection_f1", "loss", "fire_dice"]
            titles_to_plot = ["Fire IoU", "Detection F1", "Loss", "Fire Dice"]
            ylabels_to_plot = ["IoU", "F1 Score", "Loss", "Dice"]
            
            # Filter to only available metrics
            available = [(m, t, y) for m, t, y in zip(metrics_to_plot, titles_to_plot, ylabels_to_plot)
                        if all(m in df.columns for df in models_data.values())]
            
            if available:
                metrics_to_plot, titles_to_plot, ylabels_to_plot = zip(*available)
                plot_multiple_metrics_comparison(
                    models_data,
                    metrics=list(metrics_to_plot),
                    titles=list(titles_to_plot),
                    ylabels=list(ylabels_to_plot),
                    output_dir=args.output_dir,
                    split=args.split,
                )

            # Print comparison summary
            metrics_for_summary = ["fire_iou", "fire_dice", "detection_f1", "detection_recall", "loss"]
            print_comparison_summary(models_data, metrics_for_summary)

    print(f"\n\nAnalysis complete.")
    print(f"Plots saved to: {args.output_dir}")


if __name__ == "__main__":
    main()

